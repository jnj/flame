#-----------------------------------------------------------------------------
# plex.py
#
# Author: David M. Beazley (beazley@cs.uchicago.edu)
#         Department of Computer Science
#         University of Chicago
#
# Copyright (C) 2001, University of Chicago
#
# $Header: /cvs/projects/PyParse/plex/plex.py,v 1.13 2001/04/17 19:03:53 beazley Exp $
#
# This module automatically constructs a lexical analysis module from regular
# expression rules defined in a user-defined module.  The idea is essentially the same
# as that used in John Aycock's Spark framework, but the implementation works
# at a module level rather than requiring the use of classes and OO.
#
# The primary goal of this module is to provide an interface that is
# very closely modeled after the traditional lex interface in Unix.
# This implementation provides a couple of distinctions from Spark
#
#   -  It provides more extensive error checking and reporting if
#      the user supplies a set of regular expressions that can't
#      be compiled or if there is any other kind of a problem in
#      the specification.
#
#   -  The interface is better suited to LALR(1) and LR(1) parser
#      generators and situations where the parser might want to
#      alter the behavior of the scanner based on what is being
#      parsed.
#
# There are a few limitations of this module
#
#   -  It only supports one lexer at a time.  This is rarely a concern
#      for most normal compiler projects.
#
#   -  It does not support more advanced features of lex such as
#      pushback, yyless(), yymore(), yywrap() and so forth.
#
#   -  The lexer requires that the entire input text be read into
#      a string before scanning.  I suppose that most machines have
#      enough memory to make this a non-issue, but it differs from lex.
#
#-----------------------------------------------------------------------------

r"""
plex.py

This module can be used to build lex-like scanners based on regular
expressions.  To use the module, simply write a collection of regular
expression rules and actions like this:

# lexer.py
import plex

# Define a list of valid tokens
tokens = (
    'IDENTIFIER', 'NUMBER', 'PLUS', 'MINUS'
    )

# Define rules for the above tokens
def l_IDENTIFIER(t):
    r' ([a-zA-Z_](\w|_)* '
    return t

def l_NUMBER(t):
    r' \d+ '
    return t

# Some simple rules with no actions
l_PLUS = r'\+'
l_MINUS = r'-'

# Initialize the lexer
plex.lex()

The tokens list is required and contains a complete list of all valid
token types that the lexer is allowed to produce.

Rules are defined by writing a function with a name of the form
l_rulename.  Each rule must accept a single argument which is
a provisional token generated by the lexer when it recognizes a
pattern.  This token has the following attributes:

    t.type   = type string of the token.  This is initially set to the
               name of the rule without the leading l_
    t.value  = The value of the lexeme.
    t.lineno = The value of plex.lineno when the token was encountered
    
For example, the l_NUMBER() rule above might be called with the following:
    
    t.type  = 'NUMBER'
    t.value = '42'
    t.lineno = 3

Each rule returns the token object it would like to supply to the
parser.  In most cases, the token t is returned with few, if any
modifications.  To discard a token for things like whitespace or
comments, simply return nothing.  For instance:

def l_whitespace(t):
    r' \s+ '
    pass

For faster lexing, you can also define this in terms of the ignore set like this:

l_ignore = ' \t'

The characters in this string are ignored by the lexer. Use of this feature can speed
up parsing significantly since scanning will immediately proceed to the next token.

plex requires that the token returned by each rule has an attribute
t.type.   Other than this, rules are free to return any kind of
token that they wish and may construct a new type of token object
from the attributes of t (provided the new object has the required
type attribute).

If illegal characters are encountered, the scanner executes the
function l_error(t) where t is a token representing the rest of the
string that hasn't been matched.  If this function isn't defined, a
ScanError exception is raised.  The .text attribute of this exception
object contains the part of the string that wasn't matched.

The skip(n) function can be used to skip ahead n characters in the
input stream.  This is usually only used in the error handling rule.
For instance, the following rule would print an error message and
continue:

def l_error(t):
    print "Illegal character in input %s" % t.value[0]
    plex.skip(1)

Of course, a nice scanner might wish to skip more than one character
if the input looks very corrupted.

The plex module defines a variable plex.lineno that can be used
to track the current line number in the input.  The value of this
variable is not modified by plex so it is up to your lexer module
to correctly update its value depending on the lexical properties
of the input language.  To do this, you might write rules such as
the following:

def l_newline(t):
    r' \n+ '
    plex.lineno += t.value.count("\n")

To initialize your lexer so that it can be used, simply call the plex.lex()
function in your rule file.  If there are any errors in your
specification, warning messages or an exception will be generated to
alert you to the problem.

To use the newly constructed lexer from another module, simply do
this:

    import plex
    import lexer
    plex.input("position = initial + rate*60")

    while 1:
        token = plex.token()       # Get a token
        if not token: break        # No more tokens
        ... do whatever ...

Assuming that the module 'lexer' has initialized plex as shown
above, parsing modules can safely import 'plex' without having
to import the rule file or any additional imformation about the
scanner you have defined.
"""    

# -----------------------------------------------------------------------------


__version__ = "0.1"

import re, types, sys

_lexre = None                  # Master regular expression
_lexdata = None                # Actual input data (as a string)
_lexpos = 0                    # current position in input text
_lexlen = 0                    # length of input text
_lexindexfunc = [ ]            # Reverse mapping of groups to functions
_lexerrorf = None              # Error rule
_lextokens = None              # List of valid tokens
_lexignore = None              # Ignored characters

lineno = 1                     # Line number, not updated by anything!
debug  = 0                     # Debugging data

# Exception thrown when invalid token encountered and no default
class ScanError(Exception):
    def __init__(self,message,s):
         self.args = (message,)
         self.text = s

class PlexError(Exception): pass

# Stub token class if user doesn't do anything
class Token:
    def __str__(self):
        return "Token(%s,'%s',%d)" % (self.type,self.value,self.lineno)
    def __repr__(self):
        return str(self)

# -----------------------------------------------------------------------------
# lex(module)
#
# Build all of the regular expression rules from definitions in the supplied module
# -----------------------------------------------------------------------------

def lex(module=None):
    global _lexre, _lexindexfunc, _lexerrorf, _lextokens, _lexignore
    ldict = None
    regex = ""
    error = 0
    
    if module:
        if not isinstance(module, types.ModuleType):
            raise ValueError,"Expected a module"
        
        ldict = module.__dict__
        
    else:
        # No module given.  We might be able to get information from the
        # caller.
        try:
            raise RuntimeError
        except RuntimeError:
            e,b,t = sys.exc_info()
            f = t.tb_frame
            f = f.f_back           # Walk out to our calling function
            ldict = f.f_globals    # Grab its globals dictionary
        
    # Get the tokens map
    tokens = ldict.get("tokens",None)
    if not tokens:
        raise PlexError,"module does not define a list tokens"
    if not (isinstance(tokens,types.ListType) or isinstance(tokens,types.TupleType)):
        raise PlexError,"tokens must be a list or tuple."

    # Build a dictionary of valid token names
    _lextokens = { }
    for n in tokens:
        if _lextokens.has_key(n):
            print "plex: Warning. Token '%s' multiply defined." % n
        _lextokens[n] = None

    if debug:
        print "plex: tokens = '%s'" % _lextokens.keys()
        
    # Get the list of built-in functions with l_ prefix
    fsymbols = [ldict[f] for f in ldict.keys()
               if (isinstance(ldict[f],types.FunctionType) and ldict[f].__name__[:2] == 'l_')]

    # Sort the symbols by line number
    fsymbols.sort(lambda x,y: cmp(x.func_code.co_firstlineno,y.func_code.co_firstlineno))

    # Get the list of strings with the l_prefix or i_prefix
    ssymbols = [(f,ldict[f]) for f in ldict.keys()
                if (isinstance(ldict[f],types.StringType) and f[:2] == 'l_')]

    ssymbols.sort(lambda x,y: (len(x[1]) < len(y[1])) - (len(x[1]) > len(y[1])))
    
    # Check for non-empty symbols
    if len(fsymbols) == 0 and len(ssymbols) == 0:
        raise PlexError,"no rules of the form l_rulename are defined."

    # Add all of the rules defined with actions first
    for f in fsymbols:
        
        line = f.func_code.co_firstlineno
        file = f.func_code.co_filename

        if f.func_code.co_argcount > 1:
            print "%s:%d. Rule '%s' has too many arguments." % (file,line,f.__name__)
            error = 1
            continue

        if f.func_code.co_argcount < 1:
            print "%s:%d. Rule '%s' requires an argument." % (file,line,f.__name__)
            error = 1
            continue

        if f.__name__ == 'l_ignore':
            print "%s:%d. Rule '%s' must be defined as a string." % (file,line,f__name__)
            error = 1
            continue
        
        if f.__name__ == 'l_error':
            _lexerrorf = f
            continue

        if f.__doc__:
            try:
                c = re.compile(f.__doc__, re.VERBOSE)
            except re.error,e:
                print "%s:%d. Invalid regular expression for rule '%s'. %s" % (file,line,f.__name__,e)
                error = 1
                continue

            # Okay. The regular expression seemed okay.  Let's append it to the master regular
            # expression we're building
  
            if debug:
                print "plex: Adding rule %s -> '%s'" % (f.__name__,f.__doc__)
            if (regex): regex += " | "
            regex += " (?P<%s>%s) " % (f.__name__,f.__doc__)
        else:
            print "%s:%d. No regular expression defined for rule '%s'" % (file,line,f.__name__)

    # Now add all of the simple rules
    for name,r in ssymbols:
        if name == 'l_error':
            print "Rule 'l_error' must be defined as a function!"
            error = 1
            continue
        if name == 'l_ignore':
            _lexignore = r
            continue
        
        if not _lextokens.has_key(name[2:]):
            print "Rule '%s' defined for an unspecified token %s." % (name,name[2:])
            error = 1
            continue
        try:
            c = re.compile(r,re.VERBOSE)
        except re.error,e:
            print "Invalid regular expression for rule '%s'. %s" % (name,e)
            error = 1
            continue
        if debug:
            print "plex: Adding rule %s -> '%s'" % (name,r)
        if regex: regex += " | "
        regex += " (?P<%s>%s) " % (name,r)
        
    try:
        if debug:
            print "plex: regex = '%s'" % regex
        _lexre = re.compile(regex, re.VERBOSE)

        # Build the index to function map for the matching engine
        _lexindexfunc = [ None ] * (max(_lexre.groupindex.values())+1)
        for f,i in _lexre.groupindex.items():
            handle = ldict[f]
            if isinstance(handle,types.FunctionType):
                _lexindexfunc[i] = handle
            else:
                # If rule was specified as a string, we build an anonymous
                # callback function to carry out the action
                _lexindexfunc[i] = f[2:]

    except re.error,e:
        print "Fatal error. Unable to compile regular expression rules. %s" % e
        error = 1
    if error:
        raise RuntimeError,"Unable to build lexer."
    if not _lexerrorf:
        print "plex: Warning. no l_error rule is defined."
        


# -----------------------------------------------------------------------------
# input()
#
# Push a source string into the scanner.  We'll save it internally and use to 
# get tokens one by one
# -----------------------------------------------------------------------------

def input(s):
    global _lexdata, _lexpos, _lexlen

    if not isinstance(s,types.StringType):
           raise ValueError, "Expected a string"
    _lexdata = s
    _lexpos = 0
    _lexlen = len(_lexdata)

# -----------------------------------------------------------------------------
# token()
#
# Return a single token from the scanner
# -----------------------------------------------------------------------------

def token():
    global _lexpos
    
    if not _lexre:
        raise RuntimeError, "No scanner has been built with lex()."
    if not _lexdata:
        raise RuntimeError, "No data was given with input()."

    while 1:
        if _lexpos >= _lexlen:
            _lexpos += 1          # Guarantee that we are greater (useful below)
            return None           # No more input data
        if _lexignore:
            while _lexpos < _lexlen:
                if _lexdata[_lexpos] not in _lexignore: break
                _lexpos += 1
            else:
                return None
        
        # Okay.  Look for a regular expression match
        m = _lexre.match(_lexdata,_lexpos)
        if m:
            i = m.lastindex
            func = _lexindexfunc[i]
            _lexpos = m.end()

            tok = Token()
            tok.value = m.group(i)
            tok.lineno = lineno
            
            # Build a provisional token for the user
            if isinstance(func,types.FunctionType):
                tok.type = func.__name__[2:]
                tok = func(tok)
            else:
                tok.type = func
                
            # Every function must return a token, if nothing, we move to next token
            if not tok: continue

            # Verify type of the token.  If it's not in the token map, we raise
            # a run time error
            
            if not _lextokens.has_key(tok.type):
                raise PlexError,"%s:%d. Rule '%s' returned an unknown token type '%s'" % (
                    func.func_code.co_filename, func.func_code.co_firstlineno,
                    func.__name__, tok.type)
                
            return tok
    
        # Hmmm. No match
        
        if _lexerrorf:
            tok = Token()
            tok.value = _lexdata[_lexpos:]
            tok.lineno = lineno
            tok.type = "error"
            oldpos = _lexpos
            tok = _lexerrorf(tok)
            if oldpos == _lexpos:
                  # Error method didn't change text position at all
                  raise ScanError, ("Scanning error. Illegal character '%s'" % (_lexdata[_lexpos]), _lexdata[_lexpos:])
            if not tok: continue
            return tok

        raise ScanError, ("No match found", _lexdata[_lexpos:])

# -----------------------------------------------------------------------------
# skip()
#
# Skip ahead n characters in the input stream
# -----------------------------------------------------------------------------

def skip(n):
    global _lexpos
    _lexpos += n

# -----------------------------------------------------------------------------
# run()
#
# This runs the lexer as a main program
# -----------------------------------------------------------------------------

def runmain():
    try:
        filename = sys.argv[1]
        f = open(filename)
        data = f.read()
        f.close()
    except IndexError:
        print "Reading from standard input (type EOF to end):"
        data = sys.stdin.read()

    input(data)
    while 1:
        tok = token()
        if not tok: break
        print "(%s,'%s',%d)" % (tok.type, tok.value, tok.lineno)
        
    
